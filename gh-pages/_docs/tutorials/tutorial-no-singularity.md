---
title: Tutorial without Singularity
---

[HOME](./index.html)

# Data processing without Singularity

{:toc}

This tutorial takes you through the steps of running the MIMA pipeline. The pipeline requires the following setup:

**Compute environment**
- OpenPBS system on a high-performance cluster (HPC)
- Your HPC setup needs to allow running of Singularity containers

**Reference databases**
- Many of the data processing steps require access to reference databases that are too big to be included in the Singularity container
- You will need to have these already installed on your system or HPC environment and tell the pipeline of the location (this will be detailed in the relevant steps)


**Study data assumptions**
- Paired-end sequencing with two files _R1 and _R2 files
- By default the pipeline assumes human-host metagenomics studies and decontamination is done against the human genome, you can provide alternative references (see the tutorial or command usage documentation)



## The pipeline

The pipeline consists of the following components which are shown in the schema and briefly described below.

![]({{ site.baseurl}}/assets/img/tutorials/no-singularity/tut_OverallSchema.png)

**Data processing**
1) Quality control (QC) of the sequenced reads
2) Taoxonomy profiling after QC (this step can be run in paralle with step 3)
3) Functional profilinng after QC (this step can be run in parallel with step 2)

In steps 1 to 3, the pipeline generates PBS scripts (currently only supports OpenPBS) which then have to be submitted to the PBS manager to actually process the sequenced reads and generate the output.

**Analysis and visualisation**

4) Visualisation and core diversity analysis after all samples have been processed (either step 1 + 2, or step 1 + 3)
5) Other analyses e.g. classification

## How the tutorial works

The tutorial is step up as such:

- Brief introduction
- Command to generate PBS scripts - followed by explanation of the parameters in the command
- Command to submit the PBS scripts as jobs
- Expected outputs
- Post-processing step (some might be optional)

---

## Set up for tutorial

{% include alert.html type="info" title="Download data" content="In this tutorial we will use data from this study:" %}

- **INSERT STUDY**: they assessed a mock community sequenced by different platforms ...

- The raw reads are available from NCBI SRA [link]
- There are 56 paired-end samples, 112 fastq files **might provide a subset of data instead**
    - As the data is very big we will work with a smaller subset to speed up processing
    - You can download the fastq files using this script which requires the `sratoolkit`

**Data files**

{% include alert.html type="danger" title="to-do" content="Add files" %}

- [manifest.csv] file
- [metadata.csv] file
- Choose one:
    - [link to full SRA download (N=56 samples)]
    - [link to mini SRA download (N=X samples)]
 
**Folder structure**

This tutorial will assume the following folder structure

```
<PROJECT_PATH>
├── manifest.csv
└── raw_data/
    ├── Sample_1_R1.fastq.gz
    ├── Sample_1_R2.fastq.gz
    ├── Sample_2_R1.fastq.gz
    ├── Sample_2_R2.fastq.gz
    └── ...
```

From here on, `<PROJECT_PATH>` will refer to the root directory as depicted above. Replace this with the path to where you created the folder.

---

# STEP 1: QC module

## a) QC: introduction

Quality control checks to make sure that the sequenced reads obtained from the sequencing machine is of good quality. Bad quality reads or artefacts due to sequencing error if not removed can lead to spurious results and affect downstream analyses. There are a number of tools available for checking read quality of high-throughput read sequences.

This step must be done before Taxonomy profiling and Function profilng. 

This pipeline uses the following tools:

- [BBTool suite](https://jgi.doe.gov/data-and-tools/software-tools/bbtools/)
- [Fastp](https://github.com/OpenGene/fastp)
- [Minimap2](https://github.com/lh3/minimap2)

**Pipeline**

The PBS scripts generated by this step performs the following mini-pipeline:

![QC PBS pipeline]({{ site.baseurl }}/assets/img/tutorials/no-singularity/tut_QCpipeline.png)

1. **repair** - uses repair.sh from BBTools/BBMap tool suite - repairs the sequenced reads and outputs any singleton reads (these are orphaned reads that are missing either the forward or reverse partner read)

2. **dereplicate** - uses clumpify.sh from BBTools/BBMap tool suite and removes duplicate reads. It also clusters reads for faster downstream processing

3. **quality check** - uses fastp.sh and checks the quality of the reads and removes any reads that are of low quality, too long or too short

4. **decontamination** - uses minimap2.sh, which maps sequenced reads against a user-specified reference genome (fasta) file (e.g., human) and removes these host-reads from the data. The **cleaned** sequence output then becomes the input for the next steps: taxonomy or function profiling.

There is an optional step in the diagram (QC_report) that generates a summary report *after all* PBS scripts have been run for all samples in the study.

## b) QC: Generate PBS scripts

- In the terminal, type the following command
    - Replace `<your.email@address.com>` with your own email address
    - Replace `<PROJECT_PATH>` with where you created the folder, inside which should have the `raw_data` subfolder with the downloaded *.fastq.gz files
    - Hence forth, we will use `<PROJECT_PATH>` to refer to the root project folder
    - *Note* full pathnames are required for the input (`-i`) and output (`-o`) parameters

```
$ python3 qc_module.py -i /<PROJECT_PATH>/raw_data/ \
-o /<PROJECT_PATH>/output \
-m <PROJECT_PATH>/manifest.csv \
-e <your.email@address.com>
```
note:* the command above can all be typed on one line without the backslash `\` that is showing at the end of each line. The `\` tells the terminal that the command continues on the next line. We use this notation as limited space if this tutorial was printed.


**Required parameters**

| Parameters | Description |
|:-----------|:------------|
| `-i <input>` | must be *full path* to where the raw sequenced reads are stored (these files often have the *.fastq.gz or *.fq.gz extension). This path is used to find the *FileID_R1* and *FileID_R2* columns specified in the *manifest.csv* file provided (see below). |
| `-o <output>` | must be the *full path* to where you would like the output files to be saved. The `<output>` path will be created if it does not exists. **Note** if there are already existing subdirectories in the `<output>` path, then this step will fail. |
| `-e <email>` | email address for the PBS script so that you are notified when your PBS jobs complete (note that the current configuration generates one PBS script per sample file, so you will get an alert per job. So, if you have 100 samples in your study, you will have 100 emails) |
| `-m <manifest.csv>` | a comma-seperated file (*.csv) that has three columns with the headers: **Sample_ID, FileID_R1, FileID_R2** see the example below. *Note* the fileID_R1 and fileID_R2 are relative to the `-i <input>` path provided. |

*Manifest.csv* file example
```
Sample_ID,FileID_R1,FileID_R2
AMD-01,ERR1397928.sra_1.fastq.gz,ERR1397928.sra_2.fastq.gz
AMD-02,ERR1397929.sra_1.fastq.gz,ERR1397929.sra_2.fastq.gz
AMD-03,ERR1397930.sra_1.fastq.gz,ERR1397930.sra_2.fastq.gz
...
C-01,ERR1397940.sra_1.fastq.gz,ERR1397940.sra_2.fastq.gz
C-02,ERR1397941.sra_1.fastq.gz,ERR1397941.sra_2.fastq.gz
C-03,ERR1397942.sra_1.fastq.gz,ERR1397942.sra_2.fastq.gz
```

**Expected output**

- After this command you should get a new `output` directory within `<PROJECT_PATH>`
- Within `output/`, there will be a sub-drectory `QC_module`
    - Inside `output/QC_module` should be one PBS script (files with *.pbs extension) for each of the samples specified in the `manifest.csv` file
    - We need these PBS scripts for the next step

```
<PROJECT_PATH>
├── output/
│   └── QC_module
│       ├── ...
│       ├── AMD-12.pbs
│       ├── C-01.pbs
│       ├── ...
│       ├── CleanReads
│       └── QCReport
└── raw_data/
```

## c) QC: Submit PBS jobs

- Navigate to the `<PROJECT_PATH>/output/QC_module` (replace `<PROJECT_PATH>` with where you saved the output from step 1-b above)
- List the files in this directory, you should see one `*.pbs` file for each of your samples that was listed in the *manifest.csv* file
- Submit the job by typing the `qsub` command
- You can check the job has been submitted with `qstat`

```
$ cd <PROJECT_PATH>/output/QC_module
$ ls
$ qsub AMD-01.pbs
$ qstat -u $USER
```

- Wait until the job is submitted and complete


## d) QC: Outputs

- The output directory structure will look like this (we only show the output of two samples `AMD-12` and `C-01` in the diagram below, the `...` means *"and others"*)

```
<PROJECT_PATH>
├── output/
│   └── QC_module
│       ├── ...
│       ├── AMD-12.pbs
│       ├── AMD-12_singletons.fq.gz
│       ├── C-01.pbs
│       ├── C-01_qc_module.o2806088
│       ├── C-01_qc_module.o2806288
│       ├── ...
│       ├── CleanReads
│       │   ├── C-01_clean_1.fastq
│       │   └── C-01_clean_2.fastq
│       └── QCReport
│           ├── AMD-12.json
│           ├── AMD-12.outreport.html
│           ├── C-01.json
│           └── C-01.outreport.html
└── raw_data/
```

**Output files**

| Directory / Files | Description                                             |
|:------------------|:--------------------------------------------------------|
| output    | specified in the `--output-dir <output>` parameter set in step 1b)     |
| QC_module | contains all files and output from this step            |
| QC_module/*.pbs | are all the PBS scripts generated by step 1b). See below  **PBS logs** below   |
| QC_module/CleanReads| saves the cleaned reads after the QC mini-pipeline see 1a) introduction|
| QC_module/QCReport | output from fastp.sh tool |

**PBS logs**

- When the job is done, you should have two log files
    1. `*.e{PBS_JOBID}` - error log file when something goes wrong or might just have output messages from the processes
    2. `*.o{PBS_JOBID}` - console output from the processes
    - where `{PBS_JOBID}` is often a sequence of numbers that was your PBS job ID
    - check the outputs in both log files to ensure the job completed with no errors
- Check that you have outputs in the `CleanReads` folder before moving onto the next step
- Replace the `<output/path/>` section with what you input above in the `-o parameter`

```bash
$ cd <PROJECT_PATH>/output/QC_module/CleanReads
$ ls -lh
```

## e) (Optional) QC Report

- You can also generate a summary QC Report after *all samples* have been quality checked
- This step can be run directly from commandline and does not generate a PBS script

> Beware that if you have failed PBS log files (*.o) in your input directory, the qc_report.py module will not give a nice error and may fail. It reads in all PBS log files in the input directory including those that failed.

- In the terminal, type the following command:
    - Replace `<PROJECT_PATH>` with the location of what you saved your project

```
$ python3 qc_report.py -i /<PROJECT_PATH>/output/QC_module \
-o /<PROJECT_PATH>/output \
--manifest <PROJECT_PATH>/manifest.csv
```

- Output is a comma-seperated table file located in `<output>/QC_module/QC_report.csv` where `<output>` is the path provided in the `-o` parameter


---

# Step 2) Taxonomy profiling

## a) Taxa: introduction

Taxononmy profiling takes the cleaned sequence reads as input and matches them against a reference database of previously characterised sequences to annotate as taxa. There are many different classification tools, for example: Kraken2, Metaphlan, Clark, Centrifuge, MEGAN, and many more.

This pipeline uses Kraken2, which comes with its own reference database but you can also generate your own. In this tutorial, we will use the [GTDB](https://gtdb.ecogenomic.org/) database that has been pre-built for Kraken2.

**Pipeline**

The PBS scripts generated by this step performs the following mini-pipeline:

![Taxonomy PBS script mini-pipeline using Kraken2 classifier]({{ site.baseurl}}/assets/img/tutorials/no-singularity/tut_TAXApipeline.png)

1. Kraken2 classifies the reads to taxa
2. Bracken takes the Kraken2 output to estimate abundances for a given taxonomic rank
3. The final step (**generate table**) is performed after *all samples* have been processed. This combines the output and generates a *feature table* for a given taxonomic rank. The feature table contains the count or relative abundances of taxon X occuring in sample Y.


## b) Taxa: Generate PBS scripts

- In the terminal, type the following command
    - Replace `<your.email@address.com>` with your own email address
    - Replace all the `<PROJECT_PATH>` with where you created the folder during setup
    - *Note* full pathnames are required for the input (`-i`), output (`-o`), and reference (`--reference-path`) parameters

```
$ python3 taxa_module.py -i <PROJECT_PATH>/output/QC_module/CleanReads \
-o <PROJECT_PATH>/output \
-e <your.email@address.com> 
--reference-path /srv/scratch/mrcbio/db/GTDB/GTDB_Kraken \
--taxon-profiler kraken2 --walltime 2 --mem 300
```

**Required parameters**

| Parameters | Description       |
|------------|-------------------|
| `-i <input>`| full path to the `<PROJECT_PATH>/output/QC_module/CleanReads` directory that was generated from Step 1) QC, above. This directory should hold all the `*_clean.fastq` files |
| `-o <output>` | full path to the `<PROJECT_PATH>/output` output directory where you would like the output files to be saved, can be the same as Step 1) QC |
| `-e <email>`| address for the PBS script so that you are notified when your PBS jobs complete (note that the current configuration generates one PBS script per sample file, so you will get an alert per job. So, if you have 100 samples in your  study, you will have 100 emails) |
| `--reference-path <path/to/reference>` | is the path to the reference database that is required by the profile, in this example by Kraken2 |
| `--taxon-profiler <profiler>` | is an optional setting (default=Kraken2), as there are other profiler options available |

**PBS parameters**

| Parameters | Description       |
|------------|-------------------|
|`--walltime` | number of hours one PBS job needs for one sample. *Note* Kraken2 is very quick but it does need lots of RAM, see next parameter. |
|`--mem`      | gigabyte memory (RAM) required by this PBS job. *Note* Kraken2 needs a lot of memory so we set this to 300GB. |

**Expected output**

- After this step you should get the following PBS scripts (again, one for each sample) in the output directory `<PROJECT_PATH>/output/Taxonomy_profiling`
- **braken/** and **kraken2/** are subdirectories created by this step and will store the output files from the processes called in the PBS script

```
<PROJECT_PATH>
└── output/
    └── Taxonomy_profiling/
        ├── bracken/
        ├── featureTables/
        │   └── generate_bracken_feature_table.py
        ├── kraken2/
        ├── taxaProfile_AMD-01.pbs
        ├── taxaProfile_AMD-02.pbs
        ├── taxaProfile_AMD-03.pbs
        ├── ...
        ├── taxaProfile_C-01.pbs
        ├── taxaProfile_C-02.pbs
        ├── taxaProfile_C-03.pbs
        └── ...
```

## c) Taxa: Submit PBS jobs

- Go to `<PROJECT_PATH>/output/Taxonomy_profiling` directory
- Submit PBS script with `qsub`
- You can check the job has been submitted with `qstat`

```
$ cd <PROJECT_PATH>/output/Taxonomy_profiling
$ qsub taxaProfile_AMD-01.pbs
$ qstat -u $USER
```

## d) Taxa: Outputs

- After the PBS jobs have completed, you should get the following files for one sample
- We only show the outputs for **one** sample, *AMD-01*, in the tree below and `...` means *"and others"*

```
<PROJECT_PATH>
└── output/
    └── Taxonomy_profiling/
        ├── AMD-01_taxaAnnot.e2806731
        ├── AMD-01_taxaAnnot.o2806731
        ├── ...
        ├── bracken/
        │   ├── AMD-01_class
        │   ├── AMD-01_family
        │   ├── AMD-01_genus
        │   ├── AMD-01.k2_bracken_classes.report
        │   ├── AMD-01.k2_bracken_families.report
        │   ├── AMD-01.k2_bracken_genuses.report
        │   ├── AMD-01.k2_bracken_orders.report
        │   ├── AMD-01.k2_bracken_phylums.report
        │   ├── AMD-01.k2_bracken_species.report
        │   ├── AMD-01_order
        │   ├── AMD-01_phylum
        │   ├── AMD-01_species
        │   └── ...
        ├── featureTables/
        │   └── generate_bracken_feature_table.py
        ├── kraken2/
        │   ├── AMD-01.kraken2.output
        │   ├── AMD-01.kraken2.report
        │   └── ...
        ├── taxaProfile_AMD-01.pbs
        └── ...
```

**Output files**

| Directory / Files | Description                                             |
|:------------------|:--------------------------------------------------------|
| output    | specified in the `--output-dir <output>` parameter set in step 1b)     |
| QC_module | contains all files and output from this step            |
| QC_module/*.pbs | are all the PBS scripts generated by step 1b). See below     |
| QC_module/CleanReads| saves the cleaned reads after the QC mini-pipeline see 1a) introduction|
| QC_module/QCReport | output from fastp.sh tool |


**Profiler output**

- For details of Kraken2 output files, see their [documentation](https://github.com/DerrickWood/kraken2/wiki/Manual#output-formats)
- For details of Bracken output files, see their [documentation](https://ccb.jhu.edu/software/bracken/index.shtml?t=manual#format)

## e) Taxa: Generate taxonomic feature table(s)

- After *all samples* have been taxonomically annotated, we need to combine the estimated abundances into a single feature table
- We will combine the output from Bracken for each taxonomic ranks from Phylum to Species, so we should have 7 output files

- In the terminal, navigate to the `<PROJECT_PATH>/output/Taxonomy_profiling/featureTables` directory and there should be a file called "generate_bracken_feature_table" (line 1)
- Activate the `mpa3.7` conda environment if not already done (line 2)
- Execute this script (line 3)

```
$ cd <PROJECT_PATH>/output/Taxonomy_profiling/featureTables
$ conda activate mpa3.7
$ python3 generate_bracken_feature_table.py
```

- Inspect the output which should resemble the following tree structure
- There will be 4 output files per taxonomic rank
- We have only shown the set of output files for ranks `class` and `species` (with `...` meaning *"and others"*)

```
<PROJECT_PATH>
└── output/
    ├── QC_module/
    └── Taxonomy_profiling/
        ├── ...
        ├── bracken/
        ├── featureTables/
        │   ├── bracken_FT_class
        │   ├── bracken_FT_class_counts
        │   ├── bracken_FT_class_relAbund
        │   ├── ...
        │   ├── bracken_FT_species
        │   ├── bracken_FT_species_counts
        │   ├── bracken_FT_species_relAbund
        │   ├── combine_bracken_class.log
        │   ├── ...
        │   ├── combine_bracken_species.log
        │   └── generate_bracken_feature_table.py
        └── kraken2/
            
```

| Outputs | Description |
|:--------|:------------|
| {class}_FT_class | is the combined bracken output for the taxonomic CLASS rank, by default Bracken will estimate discrete read counts (_num columns) and relative abundances (_frac columns). This file contains both columns for each taxon. |
| {class}_FT_class_counts | splits the feature table and extracts the *counts* for the CLASS rank |
| {class}_FT_class_relAbund | splits the feature table and extracts the *relative abundance* for the CLASS rank |
| combine_bracken_{class}.log | log file output from the combine_bracken_table.py script from Bracken tool |
| generate_bracken_feature_table.py | python script to combine and the feature tables from multiple samples and then split into *counts* and *relative abunances* tables |

---

# Step 3) Functional profiling

## a) Func: introduction

Functional profiling, like taxonomy profiling, takes the cleaned sequenced reads as input and matches them against a reference database of previously charactered sequences to annotate as genes. There are different types of functional classification tools available.

This pipeline uses [HUMAnN3](https://huttenhower.sph.harvard.edu/humann/), which comes iwth its own reference databases. You will need to download these on to the HPC system you are working with if it's not already done so. If it's already downloaded, then you will need to know the paths to the reference databases for this step.

**ADD REFERENCE DATABASE INFO**

**Pipeline**

The PBS scripts generated in this step performs the following mini-pipline:

![Function mini-pipeline]({{ site.baseurl}}/assets/img/tutorials/no-singularity/tut_function_pipeline.png)

1. humann is used for processing and generates three outputs for each sample
2. The final step (**generate table**) is performed after *all samples* have been processed. This combines the output and generates a *feature table*. The feature table contains the abundance of gene/pathway X in sample Y. 


## b) Func: Generate PBS scripts

- In the terminal, type the following command
    - Replace `<your.email@address.com>` with your own email address
    - Replace all the `<PROJECT_PATH>` with where you created the folder during setup
    - *Note* full pathnames are required for the input (`-i`), output (`-o`), and reference paths parameters

```
$ python3 func_profiling.py -i /<PROJECT_PATH>/output/QC_module/CleanReads \
-o /<PROJECT_PATH>/output \
-e <your.email@address.com> \
--reference-path \
--function-profiler humann3 \
--walltime 12 --mem 32 --threads 28
```

**Required parameters**

| Parameters | Description       |
|------------|-------------------|
| `-i <input>`| full path to the `<PROJECT_PATH>/output/QC_module/CleanReads` directory that was generated from Step 1) QC, above. This directory should hold all the `*_clean.fastq` files |
| `-o <output>` | full path to the `<PROJECT_PATH>/output` output directory where you would like the output files to be saved, can be the same as Step 1) QC |
| `-e <email>`| address for the PBS script so that you are notified when your PBS jobs complete (note that the current configuration generates one PBS script per sample file, so you will get an alert per job. So, if you have 100 samples in your  study, you will have 100 emails) |
| `--reference-path <path/to/reference>` | is the path to the reference database that is required by the profile, in this example by Kraken2 |
| `--function-profiler <profiler>` | is an optional setting (default=humann3), as there are other profiler options available |

**PBS parameters**

| Parameters | Description       |
|------------|-------------------|
|`--walltime` | number of hours one PBS job needs for one sample. *Note* Kraken2 is very quick but it does need lots of RAM, see next parameter. |
|`--mem`      | gigabyte memory (RAM) required by this PBS job. *Note* Kraken2 needs a lot of memory so we set this to 300GB. |

**Expected output**

- After this step you should get the following PBS scripts (again, one for each sample) in the output directory `<PROJECT_PATH>/output/Function_profiling`

```
<PROJECT_PATH>
└── output/
    ├── Function_profiling/
    │   ├── AMD-01.pbs
    │   ├── AMD-02.pbs
    │   ├── ...
    │   ├── C-11.pbs
    │   └── featureTables/
    └── ...

```

## c) Func: Submit PBS jobs

- Go to <PROJECT_PATH>/output/Function_profiling directory
- Submit PBS script with `qsub`
- You can check the job has been submitted with `qstat`

```
$ cd <PROJECT_PATH>/output/Taxonomy_profiling
$ qsub AMD-01.pbs
$ qstat -u $USER
```

## d) Func: Outputs

- After the PBS jobs have completed, you should get the following files
- We only show the outputs for **one** sample, *AMD-01* in the tree view below

**Profiler output**

- For details of HUMAnN3 output files, see their [documentation](https://github.com/biobakery/humann#output-files)

## e) Func: Generate function feature table(s)

- After *all samples* have been functionally annotated, we need to combine the tables together
- There will be a table for
    - genefamilies
    - pathabundances
    - pathcoverages

- In the terminal, navigate to the `<PROJECT_PATH>/output/Fuctional_profiling/featureTables`
